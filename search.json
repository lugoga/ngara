[
  {
    "objectID": "posts/2021-09-02-pca-made-in-r/index.en.html",
    "href": "posts/2021-09-02-pca-made-in-r/index.en.html",
    "title": "PCA made easy in R",
    "section": "",
    "text": "I will extend the PCA analysis I explained in the A simple Principal Component Analysis (PCA) in R. If your new in this kind of analysis, I would recommend you to read that post before you proceeed with this post. A simple Principal Component Analysis (PCA) in R will familiarize with the general concept and you can easily follow later once you have a glimpse of PCA and its core function and how is done in R.\nI first load the packages I am going to use in this session. I will load several packages highlighted in the chunk below;\n\nrequire(tidyverse)\nrequire(kableExtra)\nrequire(factoextra)\nrequire(ggbiplot)\n\n\nLoad the dataset\nI use a simple and easy to understand dataset. This dataset consists of data on 120 observations sampled in Pemba and Zanzibar channel during the wet and dry season. This dataset has nine variables, two are factor (channel and season variables) and the other seven are numerical variables. I used read_csv to load the data and rearrange the order of variable with select() functions from dplyr package [@dplyr]\n\ndata = read_csv(\"../data/pangani.csv\")%>%\n  select(-c(1:2))  %>%\n  select(channel = site, season, everything()) \n\nI use descr() function from summarytools package [@summarytools] to get descriptive statistics of the numerical variables in the dataset;\n\ndata %>%\n  summarytools::descr()\n\nDescriptive Statistics  \n\n                       chl       do   nitrate       pH      po4   salinity      sst\n----------------- -------- -------- --------- -------- -------- ---------- --------\n             Mean     0.17     5.83      0.60     8.30     0.47      34.80    27.90\n          Std.Dev     0.31     0.70      0.67     0.46     0.24       0.64     0.59\n              Min     0.00     4.04      0.11     7.94     0.03      33.00    26.00\n               Q1     0.00     5.31      0.35     8.03     0.29      34.50    27.50\n           Median     0.00     5.88      0.47     8.05     0.44      34.95    27.90\n               Q3     0.17     6.23      0.64     8.50     0.65      35.00    28.20\n              Max     1.26     7.60      5.29     9.60     1.11      37.00    29.50\n              MAD     0.00     0.68      0.22     0.06     0.27       0.37     0.44\n              IQR     0.16     0.91      0.28     0.28     0.35       0.50     0.70\n               CV     1.82     0.12      1.11     0.06     0.52       0.02     0.02\n         Skewness     1.83     0.30      5.25     1.20     0.38      -0.16     0.12\n      SE.Skewness     0.22     0.22      0.22     0.22     0.22       0.22     0.22\n         Kurtosis     2.13    -0.10     31.34    -0.39    -0.43       1.65     0.29\n          N.Valid   120.00   120.00    120.00   120.00   120.00     120.00   120.00\n        Pct.Valid   100.00   100.00    100.00   100.00   100.00     100.00   100.00\n\n\n\n\nCompute the Principal Components\nPCA prefer numerical data, therefore, we need to trim off the dataset channel and season variables, because they are categorical variables. Once we have removed the categorical variables, we also need to filter variables for a particular season. I will start with the dry season. We use the filter function from dpyr [@dplyr] package to drop all observation collected during the rain season.\n\n## Dry season\ndry.season = data %>% \n  filter(season == \"Dry\") \n\nOur dataset is reduced to seven numerical variables and 60 observation collected during the dry season in Pemba and Zanzibar channel. To compute PCA, we simply parse the arguments data = dry.season and scale = TRUE in prcomp() function, which performs a principal components analysis and assign the output as dry.pca. But before running PCA, I first select numeric variables with select(where(is.numeric)).\n\n## PCA computation\ndry.pca = dry.season %>% \n  select(where(is.numeric)) %>%\n  prcomp(scale. = TRUE, center = TRUE)\n\nThen We can summarize our PCA object with summary().\n\ndry.pca %>% \n  summary()\n\nImportance of components:\n                          PC1    PC2    PC3    PC4    PC5     PC6    PC7\nStandard deviation     1.5046 1.3373 0.9955 0.8483 0.7964 0.63574 0.4459\nProportion of Variance 0.3234 0.2555 0.1416 0.1028 0.0906 0.05774 0.0284\nCumulative Proportion  0.3234 0.5789 0.7205 0.8233 0.9139 0.97160 1.0000\n\n\nWe get seven principal components, called PC1-9. Each of these explains a percentage of the total variation in the dataset. That is to say: PC1 explains 32% of the total variance, which means that nearly one-thirds of the information in the dataset can be encapsulated by just that one Principal Component. PC2 explains 25% of the variance. So, by knowing the position of a sample in relation to just PC1 and PC2, you can get a very accurate view on where it stands in relation to other samples, as just PC1 and PC2 can explain 57% of the variance.\n\n\ntidy approach of the result\nDavid Robinson, Alex Hayes and Simon Couch [-@broom] developed a broom package that allows to convert statistical results into tidy tibbles. that is to say the broom package takes the messy output results from models, PCA or t.test, and turns them into tidy tibbles.\nbroom package attempt to bridge the gap from untidy outputs of predictions and estimations to the tidy data we want to work with. broom is particularly designed to work with Hadley’s dplyr package [@dplyr]. In a nutshell, When we do PCA, Our focus is centered to explore the;\n\ndata in PC coordinates.\nrotation matrix.\nvariance explained by each PC.\n\n\nData in PC Coordinate\nThe rotation matrix is stored as dry.pca$rotation, but here we’ll extract it using the tidy() function from broom. When applied to prcomp objects, the tidy() function takes an additional argument matrix, which we set to matrix = \"rotation\" to extract the rotation matrix.\n\ndry.pca %>%\n  broom::tidy(matrix = \"rotation\")\n\n# A tibble: 49 x 3\n   column    PC   value\n   <chr>  <dbl>   <dbl>\n 1 sst        1 -0.545 \n 2 sst        2  0.123 \n 3 sst        3 -0.0544\n 4 sst        4  0.316 \n 5 sst        5  0.146 \n 6 sst        6 -0.699 \n 7 sst        7 -0.275 \n 8 pH         1  0.593 \n 9 pH         2 -0.194 \n10 pH         3  0.0245\n# ... with 39 more rows\n\n\nInstead of viewing the coordinates, you might be interested in the fitted values and residuals for each of the original points in the PCA For this, use augment, which augments the original data with information from the PCA:\n\ndry.pca %>%\n  broom::augment(dry.season)  %>%glimpse()\n\nRows: 60\nColumns: 17\n$ .rownames  <chr> \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"1~\n$ channel    <chr> \"Pemba\", \"Pemba\", \"Pemba\", \"Pemba\", \"Pemba\", \"Pemba\", \"Pemb~\n$ season     <chr> \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dry\", \"Dr~\n$ sst        <dbl> 29.5, 29.3, 28.8, 29.0, 28.6, 28.6, 28.0, 28.0, 28.5, 28.2,~\n$ pH         <dbl> 8.01, 8.02, 8.01, 8.01, 8.02, 8.00, 8.04, 8.04, 8.01, 8.03,~\n$ salinity   <dbl> 34.0, 34.8, 34.0, 34.8, 35.0, 34.8, 35.0, 35.5, 34.8, 36.0,~\n$ do         <dbl> 5.65, 5.06, 5.21, 5.06, 5.40, 5.11, 5.13, 6.06, 5.11, 5.55,~\n$ chl        <dbl> 0.00163, 0.00182, 0.00190, 0.00134, 0.00134, 0.00146, 0.000~\n$ po4        <dbl> 1.0670600, 0.8360900, 0.7899000, 0.7206100, 0.6744100, 0.62~\n$ nitrate    <dbl> 0.10866, 0.12085, 0.15745, 0.12085, 0.14525, 0.16965, 0.303~\n$ .fittedPC1 <dbl> -3.3128016, -2.2025913, -1.8998934, -1.6988577, -1.3060840,~\n$ .fittedPC2 <dbl> -0.44077004, 0.55118634, 0.02897391, 0.99291382, 0.83997366~\n$ .fittedPC3 <dbl> 0.732658833, 0.331566334, 1.262170852, 0.317429665, -0.0357~\n$ .fittedPC4 <dbl> 0.28245839, 0.04114448, 0.27015852, 0.20242301, -0.12900572~\n$ .fittedPC5 <dbl> 0.32047482, 1.02972979, 0.39836162, 0.55981611, 0.16040681,~\n$ .fittedPC6 <dbl> -0.21252088, -0.71467529, -0.26994931, -0.56760900, -0.4346~\n$ .fittedPC7 <dbl> -1.15788428, -0.44056637, -0.11623603, -0.15009189, 0.17792~\n\n\nNow, we want to plot the data in PC coordinates. In general, this means combining the PC coordinates with the original dataset, so we can color points by categorical variables present in the original data but removed for the PCA. We do this with the augment() function from broom, which takes as arguments the fitted model and the original data. The columns containing the fitted coordinates are called .fittedPC1, .fittedPC2, etc. We can plot then;\n\ndry.pca %>%\n  broom::augment(dry.season) %>%\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2, col = channel))+\n  geom_point(size = 3) +\n  ggsci::scale_color_jco()\n\n\n\n\nFitted components\n\n\n\n\n\n\nLook at the variance explained by each PC\nFinally, we’ll plot the variance explained by each PC. We can again extract this information using the tidy() function from broom, now by setting the matrix argument to matrix = \"eigenvalues.\n\ndry.pca %>%\n  broom::tidy(matrix = \"eigenvalues\")\n\n# A tibble: 7 x 4\n     PC std.dev percent cumulative\n  <dbl>   <dbl>   <dbl>      <dbl>\n1     1   1.50   0.323       0.323\n2     2   1.34   0.255       0.579\n3     3   0.996  0.142       0.720\n4     4   0.848  0.103       0.823\n5     5   0.796  0.0906      0.914\n6     6   0.636  0.0577      0.972\n7     7   0.446  0.0284      1    \n\n\nYou notice that we get a tibble format of the values, we can use these values to plot\n\ndry.pca %>%\n  broom::tidy(matrix = \"eigenvalues\") %>%\n  ggplot()+\n  geom_col(aes(x = PC, y = percent), fill = \"maroon\") +\n  geom_line(aes(x = PC, y = cumulative))+\n  geom_point(aes(x = PC, y = cumulative), size = 3) +\n  scale_y_continuous(labels = scales::percent_format(), expand = expansion(mult = c(0,0.01)))+\n  scale_x_continuous(breaks = 1:8)\n\n\n\n\nEigenValues of the seven PCA components\n\n\n\n\nThe first and second component captures 60% of the variation in the data (Figure @ref(fig:fig3)) and, as we can see from the figure @ref(fig:fig2), nicely separates the Pemba channel samples from the Zanzibar channel samples.\n\n\n\nCited materials\n\n\n\n\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ng'ara",
    "section": "",
    "text": "This blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "posts/coding_r/index.html",
    "href": "posts/coding_r/index.html",
    "title": "Welcome to Coding in R",
    "section": "",
    "text": "Coding is now in blazing flame. Welcome!\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "posts/coding_r/index.html#the-way",
    "href": "posts/coding_r/index.html#the-way",
    "title": "Welcome to Coding in R",
    "section": "The Way",
    "text": "The Way\nSuntibus sunte num est ut exeruptate de ex eveliqui nis cus derumet, nes aliquia ssequae nonserferum del ipsapic te volectiunt facest et, serum acius, aut iduciam et am nonectus sumque parum fuga. Ut acerfer itiusam lam eosae consequodit quasimusae coris remquam nimossum int vitiis rem exerovi tatqui te pa conseque nonsequae. Aborae con num et occus. Untio occum adit quiant, se veria sin custionsecto qui cus quiduci tassit harum ea sit et quiatur sit que parchil et ut et untus eatia nonsequ ideribus numquat restio beaquis expliqu iamendanim quis et lam net andiame ndanissim et labo. Sedit volorum facestiore, autet, ut quam quibus re remperu mquaernatiae sant od que corehenti commoloratio moluptatia cusamus ciatisc ienihitis ipsamet invelicae doluptam, corentius exerate culpa sit re nemodio repudigendit la aspienietur, sinvent, sequossime re dolestiis ratendest, cum evelestiame es et aut vel int, quis si tem nam est, tet omnis niet accuscienis explige ntiumque sundignis dolorum dunt."
  },
  {
    "objectID": "posts/coding_r/index.html#when-riding",
    "href": "posts/coding_r/index.html#when-riding",
    "title": "Welcome to Coding in R",
    "section": "When Riding",
    "text": "When Riding\nTur abo. Explaut vendi doloreius atinum vel modipic to es alistiberum illaut di comni del ipsum quam, utes ducid ut omni acesedi caectates ducit volestrum fugit explabo. Ihictium vit re voluptati derro minus etusda denist, sa nosanditi aut venim quis molor ra nonsequ atempel molorpos reptatibus sa nullore voluptatis cum vel is natur moluptam ideliqu aepudit assereh enihill uptaessit alit ma nonseca eperro etur, officient unt enimi, sequiat istiam ullestrum que sam, venda doloribust, serio il et ommolore dent adiae. Rundelessum hiciendus sequo et aut essi qui adipsanis voluptae perupta tuscius necearunt persper feribus esequo volorec tentur? Aliatem eius audit aborrov idenit maioribus quasperum iusam cuscipsant."
  },
  {
    "objectID": "posts/coding_r/index.html#the-killing-time",
    "href": "posts/coding_r/index.html#the-killing-time",
    "title": "Welcome to Coding in R",
    "section": "The killing time",
    "text": "The killing time\nIn consequam sequat eatinti onseque plique re endis quiatur alibus quasperit asi doleniam, quuntio eseque volorion ra quaecernatus endessum quis endipita quia dis am, conest, imporem etur sunt eaqui tenias es alicaborate vellitinimus maiorumque pre, qui reruptur aut fugia volo conseditate vel mosandae omnimet ipicatin cor as que nectisitium remporenim autatur rerum ipidunt quodis et volorio. Bis debis aut quo vereiur? Catem num hil il eos dent a doluptatur sin eaquas parchitatur, volorpos quaeruptatio quam quam qui doloritatium nostinum facea doluptati aut re dignat."
  },
  {
    "objectID": "posts/coding_r/index.html#we-love-the-flow",
    "href": "posts/coding_r/index.html#we-love-the-flow",
    "title": "Welcome to Coding in R",
    "section": "We love the flow",
    "text": "We love the flow\nMus dolorendae pe plaborrorro et ut dolut est maion re, sitint fugia dis nus nobis explici dellaborrum aspe sequae nonseru ptature mpellam, offic te por aut dolorit atecearum sus que consend untiberit ad ullaccab in commo cus, odit a vellore ssundae pore provid ut qui tota comnis conseri taspelitates a volentus et es imagnit haria voluptatios estiuntis solorum et, simus ex estiae corro blanitata nisiment et aut lacerferum aut arum quibeaquae volorem is aut atia accum quae plantis audit veni atur? Ficimus, nonescient et voluptatibus evenimet adis in nonsequatur aut quame porem fuga. Sent el moluptaes diorunt eume et quiandebit harupissit moles doluptiis et poreium quatur rest utem samet doluptur? Quias reperatis ipsum vit aut aliquiditem fugiandicit eaqui ut issi dioneceris volore volorepedi officiditia volum exped ut optas rem eati que evelictem vernam, si cus expedic totaecepelit vendam, odit lantestius, optiumquam evero et eariore pudignis ea quaepud antissime parciat ioruptaturem sum re ipitati odipis adis imolorio."
  },
  {
    "objectID": "posts/coding_r/index.html#conclusion",
    "href": "posts/coding_r/index.html#conclusion",
    "title": "Welcome to Coding in R",
    "section": "Conclusion",
    "text": "Conclusion\nAt il invel iur, cor aut eos aut haribus ut earchil iur, cum vitatust fuga. Nequaes repudae eum aciis as remporum est vollaboribus is ea volorero quas nectorios id molut labo. Itatur, ut arumquo omnienderum sunt re dicia diti occatur, sit, sum fuga. Nequias sitibusti officim aximpor epudis abor resti distior ru"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html",
    "href": "posts/forecastingTimeseries/index.html",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "",
    "text": "Time-series analysis aims to analyse and learn the temporal behaviour of datasets over a period\nTime-series analysis aims to analyse and learn the temporal behaviour of datasets over a period. Examples include the investigation of long-term records of temperature , sea-level fluctuations, the effect of the El Niño/Southern Oscillation on tropical rainfall, and surface current influences on distribution of temperature and rainfall. Th e temporal pattern of a sequence of events in a time series data can be either random, clustered, cyclic, or chaotic.\nTime-series analysis provides various tools with which to detect these temporal patterns. Moreover, it helps in learning the behavior of the dataset by plotting the time series object on the graph. In R language, time series objects are handled easily using ts() function that takes the data vector and converts it into time series object as specified in function parameters. Therefore, understanding the underlying processes that produced the observed data allows us to predict future values of the variable.\nIn this post we learn how to forecast in R. We will use the prophet package [@prophet], which contain all the necessary routines for time-series analysis. Prophet is a package developed by Facebook for forecasting time series objects or data. Prophet package is based on decomposable model i.e., trend, seasonality and holidays that helps in making more accurate predictive models. It is much better than the ARIMA model as it helps in tuning and adjusting the input parameters.\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#dataset",
    "href": "posts/forecastingTimeseries/index.html#dataset",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Dataset",
    "text": "Dataset\nWe will use the NASA GISTEMP V4 dataset that combine NOAA GHCN meteorological stations and ERSST ocean temperature to form a comprehensive long record of temperature variability of the earth surface. The dataset contains monthly temperature values from 1880 to present, which is widely used to monitor the weather and climate at regional and global scale. Rather than using absolute temperature values, the dataset uses anomaly obtained by using base period (1951-1980).\nNASA’s analysis incorporates surface temperature measurements from more than 26,000 weather stations and thousands of ship- and buoy-based observations of sea surface temperatures. These raw measurements are analyzed using an algorithm that considers the varied spacing of temperature stations around the globe and urban heating effects that could skew the conclusions if not taken into account. The result of these calculations is an estimate of the global average temperature difference from a baseline period of 1951 to 1980.\nThis dataset is open and free to download as netCDF format file at GISTEMP. I have processed the file and we can load as the csv file here.\n\nglobal = read_csv(\"../data/temperature_lss_global_1990_2020_2021.csv\")\n\nThe Earth’s global average surface temperature in 2021 tied 2018 (See Figure 1) is the sixth-warmest year on record, according to independent analyses from NASA and the National Oceanic and Atmospheric Administration (NOAA). According to scientists at NASA’s Goddard Institute for Space Studies (GISS), global temperatures in 2021 were 0.85 degrees Celsius above the average for NASA’s baseline period,\n\ntemperature = global %>% filter(year == 2021)\n\nggplot()+\n  metR::geom_contour_fill(data = temperature, aes(x = lon, y = lat, z = temperature),bins = 120)+\n  metR::geom_contour2(data = temperature, aes(x = lon, y = lat, z = temperature,label = ..level..), breaks = 0, color = \"red\")+\n  ggspatial::layer_spatial(data = World, fill = NA)+\n  coord_sf(xlim = c(-180,180), ylim = c(-70,70))+\n  # metR::scale_fill_divergent(midpoint = 0)+\n  scale_fill_gradientn(colours = mycolor3, \n                       # trans = scales::modulus_trans(p = .1),\n                       name = expression(T~(degree*C))) +\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  # metR::scale_y_latitude(ticks = 15)+\n  metR::scale_x_longitude()\n\n\n\n\nFigure 1: Global land and sea surface temperature anomaly for a year 2021 compared to to the 1950-1980 average\n\n\n\n\nRegardless of the COVID-19 pandemic that reduction in mobility and human activities, along with reduced industrial production, has led to lower levels of nitrogen dioxide (NO2) and and the subsequent decrease of fossil fuel burning and CO2 emissions, NASA found that the year 2020 (Figure 2) was the hottest year on record. Continuing the planet’s long-term warming trend, the year’s globally averaged temperature was 1.02 degrees Celsius warmer than the baseline 1951-1980 mean, according to scientists at NASA’s Goddard Institute for Space Studies (GISS) in New York. 2020 edged out 2016 by a very small amount, within the margin of error of the analysis, making the years effectively tied for the warmest year on record. The last seven years have been the warmest seven years on record.\n\ntemperature = global %>% filter(year == 2020)\n\nggplot()+\n  metR::geom_contour_fill(data = temperature, aes(x = lon, y = lat, z = temperature),bins = 120)+\n  metR::geom_contour2(data = temperature, aes(x = lon, y = lat, z = temperature,label = ..level..), breaks = 0, color = \"red\")+\n  ggspatial::layer_spatial(data = World, fill = NA)+\n  coord_sf(xlim = c(-180,180), ylim = c(-70,70))+\n  # metR::scale_fill_divergent(midpoint = 0)+\n  scale_fill_gradientn(colours = mycolor3, \n                       # trans = scales::modulus_trans(p = .1),\n                       name = expression(T~(degree*C))) +\n  theme_bw(base_size = 12)+\n  theme(axis.title = element_blank())+\n  # metR::scale_y_latitude(ticks = 15)+\n  metR::scale_x_longitude()\n\n\n\n\nFigure 2: 2020! the hottest year on record"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#data-preparation-exploration",
    "href": "posts/forecastingTimeseries/index.html#data-preparation-exploration",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Data Preparation & Exploration",
    "text": "Data Preparation & Exploration\nProphet works best with periodicity data with at least one year of historical data. It’s possible to use Prophet to forecast using sub-daily or monthly data, but for the purposes of this post, we’ll use the monthly periodicity of global temperature—land and ocean . Let’s us read the file into our session\n\nmonthly = read_csv(\"../data/temperature_lss.csv\")\n\nmonthly %>% FSA::headtail(n = 5)\n\n           time temperature\n1    1880-01-15  -0.2322751\n2    1880-02-15  -0.4008110\n3    1880-03-15  -0.1818604\n4    1880-04-15  -0.2003687\n5    1880-05-15  -0.1266611\n1701 2021-09-15   1.1336590\n1702 2021-10-15   1.3624321\n1703 2021-11-15   1.0853070\n1704 2021-12-15   0.9597652\n1705 2022-01-15   1.1144421\n\n\nLooking on the printed dataset, we note that we have records of land and sea temperature since January 1880 through January 2022. That is the long historical data that suits our analysis. The first thing we need to do is to create a time-series object in R. This is done by using a ts function and specify the start year and the frequency of observation. Since we have monthly records, the frequency for each year will be 12 as the chunk highlight\n\nts.temp = monthly %>% \n  pull(temperature) %>% \n  ts(start = c(1880,1), frequency = 12)\n\nBut, although we have that long historical dataset, we are more interested in the most recent records. Therefore, we filter all records since 1980 for our analysis. We can achieve this by simply passing the limiting year and month in the window function. Once we have filtered the dataset, we then convert the ts object to prophet format using a ts_to_prophet function from TSstudio package\n\nts.df = ts.temp %>% \n  window(start = c(1980,1)) %>% \n  TSstudio::ts_to_prophet()\n\nTracking global temperature trends provides a critical indicator of the impact of human activities— specifically, greenhouse gas emissions – on our planet. Visualizing the dataset as seen in figure figure @ref(fig:trend). It’s an undeniable fact the global mean temperature is constantly rising. Earth’s average temperature has risen above 1.2 degrees Celsius) since the late 19th century and the IPCC has pointed out the increase should be limited to 1.5 °C above pre-industrial levels, to have any hope of mitigating the harmful effects of climate change.\n\nts.df %>% \n  ggplot(aes(x = ds, y = y))+\n  geom_line()+\n  geom_smooth(fill = \"red\", color = \"red\", alpha = .2)+\n  scale_y_continuous(name = expression(Temperature~(degree*C)))+\n  scale_x_date(date_breaks = \"3 year\", labels = scales::label_date_short())+\n  theme_bw(base_size = 12)+\n  theme(axis.title.x = element_blank(), panel.grid.minor = element_blank())\n\n\n\n\nTrend of global temperature"
  },
  {
    "objectID": "posts/forecastingTimeseries/index.html#forecasting",
    "href": "posts/forecastingTimeseries/index.html#forecasting",
    "title": "Forecasting Rising Temperature with prophet package in R",
    "section": "Forecasting",
    "text": "Forecasting\nThe first step in creating a forecast using Prophet is importing the prophet library into our R session. Once the prophet library is loaded into our session, we’re ready to fit a model to our historical data. We can achieve that by simply calling theprophet() function using your prepared dataframe as an input:\n\nm = ts.df %>% prophet()\n\nOnce we have used Prophet to fit the model dataset, we can now start making predictions for future dates. Prophet has a built-in helper function make_future_dataframe to create a dataframe of future dates. The make_future_dataframe function, which allows to specify the frequency and number of periods we would like to forecast into the future. By default, the frequency is set to days. Since we are using daily periodicity data in this example, we will leave freq at it’s default and set the periods argument to 365, indicating that we would like to forecast 365 days into the future.\n\nfuture = m %>% \n  make_future_dataframe(120,freq = \"month\")\n\nWe can now use the predict() function to make predictions for each row in the future dataframe.\n\nforecast = m %>% predict(future)\n\nAt this point, Prophet will have created a new dataframe assigned to the forecast variable that contains the forecasted values for future dates under a column called yhat, as well as uncertainty intervals and components for the forecast. We can visualize the forecast using Prophet’s built-in plot helper function:\n\nplot(x = m, fcst = forecast, uncertainty = T,plot_cap = T,)+\n  scale_y_continuous(name = expression(Temperature~(degree*C)))+\n  # scale_x_date(date_breaks = \"3 year\", labels = scales::label_date_short())+\n  theme_bw(base_size = 12)+\n  theme(axis.title.x = element_blank())+\n  add_changepoints_to_plot(m = m, threshold = 0.1)\n\n\n\n\nIf we want to visualize the individual forecast components, we can use Prophet’s built-in plot_components function:\n\nplot_forecast_component(m = m, fcst = forecast, name = \"trend\")"
  },
  {
    "objectID": "posts/lessons/index.html",
    "href": "posts/lessons/index.html",
    "title": "Data Science at the frontline",
    "section": "",
    "text": "Data science has become an essential element for companies that want to …………….\nData science has become an essential element for companies that want to gain a competitive advantage. The role of data science is to put the data analytics process into a strategic context so that companies can harness the power of their data while working on their data science project. In this post I will take you through some key concepts that a keen data scientists should carefully and with intent consider and explore before take a data related consutation. A clear understanding of these key concept will guide into some key focus areas that surround the project. You will understand the differences between different stages and how to tackle them effectively depending on your end goal with the project.\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "posts/lessons/index.html#time-sinks-for-data-wrangling",
    "href": "posts/lessons/index.html#time-sinks-for-data-wrangling",
    "title": "Data Science at the frontline",
    "section": "Time sinks for data wrangling",
    "text": "Time sinks for data wrangling\nEstimating the time it will take to complete a project is an important part of being a successful PhD student, researcher or consultant. A major time sink on many analytical projects is data wrangling. Often the wrangling can take longer than the analysis. Here’s some good questions to ask yourself and your collaborators. They will help you better estimate the time you need for data wrangling."
  },
  {
    "objectID": "posts/lessons/index.html#who-has-the-data",
    "href": "posts/lessons/index.html#who-has-the-data",
    "title": "Data Science at the frontline",
    "section": "Who has the data?",
    "text": "Who has the data?\nIf you are starting a collaboration, find out where the data is stored before starting. Many times collaborators have told me they ‘have’ or ‘can get’ the data. We start and I find out they don’t actually, literally, have the data, they just know someone who has it. So begins a lengthy negotiation about data access with the person that actually has the data. Another consideration in such cases is whether your use is consistent with original ethics approval for the data collection (if it was needed)."
  },
  {
    "objectID": "posts/lessons/index.html#is-the-data-real",
    "href": "posts/lessons/index.html#is-the-data-real",
    "title": "Data Science at the frontline",
    "section": "Is the data real?",
    "text": "Is the data real?\nSeriously, if its not your data, are you sure its real data? Ok this situation is rare (I hope), but it does happen that collaborators share false data."
  },
  {
    "objectID": "posts/lessons/index.html#how-is-the-data-recorded-and-stored",
    "href": "posts/lessons/index.html#how-is-the-data-recorded-and-stored",
    "title": "Data Science at the frontline",
    "section": "How is the data recorded and stored?",
    "text": "How is the data recorded and stored?\nIf collaborating to get data, find out how it is stored and recorded. Is it in pdf tables, is it a single spreadsheet with good meta-data, or is it a series of nested excel sheets with cross-referenced formulas?"
  },
  {
    "objectID": "posts/lessons/index.html#has-the-data-been-analyzed-before",
    "href": "posts/lessons/index.html#has-the-data-been-analyzed-before",
    "title": "Data Science at the frontline",
    "section": "Has the data been analyzed before?",
    "text": "Has the data been analyzed before?\nIf it has, it is usually already cleaned and easier to work with. Also check what it has been used for. If it has been used just to generate high level summary stats, it might not be clean for detailed analysis. If its been analyzed in R or python before, even better. It is probably clean and R ready."
  },
  {
    "objectID": "posts/lessons/index.html#how-big-ie-computer-memory-is-the-data",
    "href": "posts/lessons/index.html#how-big-ie-computer-memory-is-the-data",
    "title": "Data Science at the frontline",
    "section": "How big (ie computer memory) is the data?",
    "text": "How big (ie computer memory) is the data?\nBigger datasets are harder to clean and error check. Very small data can be checked manually. Larger datasets can easily be checked with a combination of manual checking, graphs and programming. Very large dataset (ie bigger than your RAM, like gigabytes) present new problems. R will take a while to chug through processing steps. This can really slow down debugging, especially for complex tasks.\nYou may also have to develop special strategies, like breaking your data into chunks for error checking and debugging of analysis. All in all this means a 500kb file takes about the same amount of time to wrangle as a 100mb file, whereas a 1 gigabyte file might take 10 times longer."
  },
  {
    "objectID": "posts/lessons/index.html#does-the-project-involve-spatial-data",
    "href": "posts/lessons/index.html#does-the-project-involve-spatial-data",
    "title": "Data Science at the frontline",
    "section": "Does the project involve spatial data?",
    "text": "Does the project involve spatial data?\nSpatial data records dependencies between data points (coordinates). This can present new problems. You’ll obviously need to use specialist GIS tools. Processing steps can be much slower, because the algorithms to cope with spatial dependency just take longer. For example, the maths of a spatial join is much more complex (geometry) than that for a regular join of two dataframes on a single variable."
  },
  {
    "objectID": "posts/lessons/index.html#does-the-project-involve-temporal-data",
    "href": "posts/lessons/index.html#does-the-project-involve-temporal-data",
    "title": "Data Science at the frontline",
    "section": "Does the project involve temporal data?",
    "text": "Does the project involve temporal data?\nDates and times are hard to work with. First there can be gaps in a time-series. How will you deal with that? Maybe you need to impute values before analysis?\nSecond, the programming of dates is just hard. I don’t know anyone that likes it. For example, what day is 30 days before the 15th March? Depends on whether its a leap year right? Annoying details like this can really slow you dowe.\nAlso timezones! People always send me data with times, but neglect to tell me what time zone its in (or I forget to ask perhaps)."
  },
  {
    "objectID": "posts/lessons/index.html#are-you-joining-multiple-datasets",
    "href": "posts/lessons/index.html#are-you-joining-multiple-datasets",
    "title": "Data Science at the frontline",
    "section": "Are you joining multiple datasets?",
    "text": "Are you joining multiple datasets?\nThis is a big one. Joining datasets is a major, major, timesink.\nSo much so, you could basically say my niche in my field is joining different datasets in new ways. We can get new interesting results, but it can be hard (which is why not everyone is doing it).\nFirst, joins present opportunities for errors. If you are doing a simple join on a shared variable (e.g. with dplyr’s inner_join), but the variable is coded inconsistently across dataframes you might loose data. This might be hard to detect in large datasets.\nSecond, joins might be based more loosely on spatial or temporal similarities. These can get fiddly. Say you want to extract wave height data at some diver survey sites. Just use the coordinates right? Well, what if the wave data doesn’t extend all the way to the coast, or the resolution is too ‘blocky’ and it misses some dive sites? Now you have to invent a way to find wave values near to dive sites, ideally only when wave data are missing. It gets fiddly.\nThe same issue happens with joins on times/dates. Maybe rainfall isn’t measured on the exact day you had bird counts. So perhaps now you impute rainfall to the missing day, but what algorithm of the 1000s possible are you going to use for the imputation?"
  },
  {
    "objectID": "posts/lessons/index.html#summary",
    "href": "posts/lessons/index.html#summary",
    "title": "Data Science at the frontline",
    "section": "Summary",
    "text": "Summary\nSo think carefully about the issues your project’s data might present. If you’re still not sure how long it might take, then try this formula recommended by a friend:"
  },
  {
    "objectID": "posts/newvisuals/index.html",
    "href": "posts/newvisuals/index.html",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "",
    "text": "The visual impact of the flow diagram has caught the form of looking…….\nOn April, 2022, the government of the United Republic of Tanzania approved permission of TAMISEMI to recruit and employ 9,800 primary and secondary school teachers, and 7,612 health experts. A total of 165,948 applied for the positions where for Health Cadres is 42,558, and the Teaching Cadre is 123,390.\nAllocation of health and Teaching position in Centers of providing health services and schools have considered the requirements of employees in the respective regions. The allocation of new employees was based on the needs of employees in Councils with new Hospitals, Centers New health, and new completed clinics that faced shortage of medical staffs.\nIn addition, teachers were allocated to councils based on the division of spacevfor each subject, and the qualifications. In this post, we are going to discuss\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "posts/newvisuals/index.html#tilemap",
    "href": "posts/newvisuals/index.html#tilemap",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Tilemap",
    "text": "Tilemap\n\ndistrict.tb %>% \n  hchart(type = \"tilemap\", hcaes(x = lon, y = lat, name = district, group = zone)) %>% \n  hc_chart(type = \"tilemap\") %>% \n  hc_plotOptions(\n    series = list(\n      dataLabels = list(\n        enabled = TRUE,\n        format = \"{point.code}\",\n        color = \"white\",\n        style = list(textOutline = FALSE)\n      )\n    )\n  ) %>% \n  hc_tooltip(\n    headerFormat = \"\",\n    pointFormat = \"<b>{point.name}</b> is in <b>{point.region_nam}</b>\"\n    ) %>% \n  hc_xAxis(visible = FALSE) %>% \n  hc_yAxis(visible = FALSE) %>% \n  hc_size(height = 800, width = 600)"
  },
  {
    "objectID": "posts/newvisuals/index.html#packedbubble",
    "href": "posts/newvisuals/index.html#packedbubble",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "packedbubble",
    "text": "packedbubble\nA bubble chart requires three dimensions of data; the x-value and y-value to position the bubble along the value axes and a third value for its volume. Packed Bubble charts have a simpler data structure, a flat, one-dimensional array with volumes is sufficient. The bubble’s x/y position is automatically calculated using an algorithm that packs the bubbles in a cluster. The series data point configuration has support for setting colors and label values. Drag’n drop feature was also added to give the user a chance to quickly move one bubble between series and then check how their relations will change.\n\n\n\n\nwalimu.lga = walimu.clean %>% \n  separate(halmashauri, into = c(\"district\", \"b\", \"c\"), sep = \" \") %>% \n  unite(col = code, b:c, sep = \" \") %>% \n  mutate(lga = case_when(code == \"District Council\"~\"DC\",\n                         code == \"Municipal Council\"~\"MC\",\n                         code == \"City Council\"~\"CC\",\n                         code == \"Town Council\"~\"TC\",\n                         code == \"Mikindani Municipal\"~\"MC\",\n                         code == \"Ujiji Municipal\"~\"MC\"))\n\n  \nwalimu.lga.freq = walimu.lga %>% \n  group_by(district, lga) %>% \n  count()\n\ndistrict.walimu = district.tb %>% \n  left_join(walimu.lga.freq) %>% \n  select(region_nam, zone, n, district)%>% \n  separate(district, into = c(\"code\", \"aa\"), sep = 3, remove = FALSE) %>% \n  mutate(code = str_to_upper(code)) %>% \n  select(-aa)\n\n\nhc = district.walimu %>% \n   hchart(type = \"packedbubble\", hcaes(name = district, value = n, group = zone))\n\n\n\nq95 <- as.numeric(quantile(district.walimu$n, .95, na.rm = TRUE))\n\nhc %>% \n  hc_tooltip(\n    useHTML = TRUE,\n    pointFormat = \"<b>{point.name}:</b> {point.n}\"\n  ) %>% \n  hc_plotOptions(\n    packedbubble = list(\n      maxSize = \"150%\",\n      zMin = 0,\n      layoutAlgorithm = list(\n        gravitationalConstant =  0.05,\n        splitSeries =  TRUE, # TRUE to group points\n        seriesInteraction = TRUE,\n        dragBetweenSeries = TRUE,\n        parentNodeLimit = TRUE\n      ),\n      dataLabels = list(\n        enabled = TRUE,\n        format = \"{point.code}\",\n        filter = list(\n          property = \"y\",\n          operator = \">\",\n          value = q95\n        ),\n        style = list(\n          color = \"black\",\n          textOutline = \"none\",\n          fontWeight = \"normal\"\n        )\n      )\n    )\n  )"
  },
  {
    "objectID": "posts/newvisuals/index.html#sankey",
    "href": "posts/newvisuals/index.html#sankey",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Sankey",
    "text": "Sankey\nA sankey diagram is a visualization used to depict a flow from one set of values to another. The things being connected are called nodes and the connections are called links.Sankey diagrams can also visualize the energy accounts, material flow accounts on a regional or national level, and cost breakdowns.[1] The diagrams are often used in the visualization of material flow analysis.\nSankey diagrams emphasize the major transfers or flows within a system. They help locate the most important contributions to a flow. They often show conserved quantities within defined system boundaries.\n\n   quest.tb =  walimu.clean %>% \n      group_by(kiwango_cha_elimu, jinsi) %>% \n      summarise(value = n(), .groups = \"drop\") %>% \n      rename(source = 2, target = 1) %>% \n      filter(value > 100)%>% \n      as.data.frame()\n        \n    \n    # From these flows we need to create a node data frame: it lists every entities involved in the flow\n    nodes <- data.frame(name=c(as.character(quest.tb$source), \n                               as.character(quest.tb$target)) %>% \n                          unique())\n    \n    nodes = quest.tb %>% \n      select(-value) %>% \n      pivot_longer(cols = source:target) %>% \n      distinct(value) %>% \n      rename(name = 1) %>% \n      as.data.frame()\n    \n    # With networkD3, connection must be provided using id, not using real name like in the links dataframe.. So we need to reformat it.\n    quest.tb$IDsource=match(quest.tb$source, nodes$name)-1 \n    quest.tb$IDtarget=match(quest.tb$target, nodes$name)-1\n    \n    \n    # Make the Network \n    networkD3::sankeyNetwork(Links = quest.tb, \n                             Nodes = nodes,\n                             Source = \"IDsource\", \n                             Target = \"IDtarget\",\n                             Value = \"value\", \n                             NodeID = \"name\", \n                             fontFamily = \"Myriad Pro\",\n                             LinkGroup = \"source\",\n                             sinksRight=FALSE,\n                             # height = 600, width = 800,\n                             # colourScale=ColourScal,\n                             nodeWidth=30, \n                             iterations = 5,\n                             fontSize=14, \n                             nodePadding=30, \n                             width = 1000, \n                             height = 400)"
  },
  {
    "objectID": "posts/newvisuals/index.html#chord-diagram",
    "href": "posts/newvisuals/index.html#chord-diagram",
    "title": "Teacher’s Employment Allocations by LGA",
    "section": "Chord diagram",
    "text": "Chord diagram\nA chord diagram represents flows or connections between several entities (called nodes). Each entity is represented by a fragment on the outer part of the circular layout. Then, arcs are drawn between each entities. The size of the arc is proportional to the importance of the flow. In this section I will discuss the transfer of public servants between region in the country. I will use the chord diagram, which makes visual appeal and provide insight in a more clear form.\n\ntamisemi = readxl::read_excel(\"d:/semba/vpo/data/tamisemi/uhamisho_data.xlsx\") %>% \n  janitor::clean_names()\n\n\ntam.clean = tamisemi %>% \n  separate(col = anakotoka, into = c(\"lga_toka\", \"bb\", \"lga_toka_name\"))%>% \n  separate(col = anakoenda, into = c(\"lga_enda\", \"bb\", \"lga_enda_name\")) %>% \n  relocate(c(lga_enda, lga_enda_name), .after = lga_toka_name) %>% \n  mutate(lga_toka = if_else(lga_toka==\"Manipaa\", \"Manispaa\", lga_toka),\n         lga_toka = if_else(lga_toka %in% c(\"Wiaya\", \"WIlaya\"), \"Wilaya\", lga_toka),\n         lga_enda = if_else(lga_enda==\"Manipaa\", \"Manispaa\", lga_enda),\n         lga_enda = if_else(lga_enda %in% c(\"Wiaya\", \"WIlaya\"), \"Wilaya\", lga_enda))\n\n\nmikoa = readxl::read_excel(\"d:/semba/vpo/data/tamisemi/uhamisho_data.xlsx\", sheet = 2) %>% \n  janitor::clean_names()\n\n\nmikoa = mikoa %>% \n  dplyr::select(-2) %>% \n  separate(col = halmashauri, into = c(\"lga\", \"desig\"), sep = \" \") %>% \n  dplyr::select(-desig)\n\n\ntoka = tam.clean %>% \n  dplyr::select(lga_toka_name) %>% \n  left_join(mikoa, by = c(\"lga_toka_name\" = \"lga\"))\n\n\nenda = tam.clean %>% \n  dplyr::select(lga_enda_name) %>% \n  left_join(mikoa, by = c(\"lga_enda_name\" = \"lga\")) %>% \n  rename(mkoa = mikoa) %>% \n  mutate(region = mkoa) %>% \n  slice(-1)\n\nLooking in all region hide crucial information that\n\nmkoa.mkoa = toka %>% bind_cols(enda) %>% \n  group_by(mikoa, region) %>% \n  count() %>% \n  datawizard::reshape_wider(values_from = \"n\", colnames_from = \"region\") %>% \n  mutate( across(contains(\"n_\"), ~ifelse(is.na(.x),0,.x))) %>% \n  slice(-27) %>% \n  dplyr::select(-n_NA) \n\nmajina = mkoa.mkoa %>% names() %>% as_tibble() %>% separate(col = 1, into = c(\"aa\", \"bb\"), sep = \"_\") %>% mutate(bb = if_else(is.na(bb), \"mikoa\", bb)) %>% pull(bb) \n\nnames(mkoa.mkoa) = majina\n\n\nmkoa.matrix = mkoa.mkoa %>% \n  column_to_rownames(var = 'mikoa') %>% \n  as.matrix() \n\n\nmkoa.matrix %>% \n  chorddiag(\n    type = \"bipartite\", \n            showTicks = F, tickInterval = 2,\n            groupnameFontsize = 14, \n            groupnamePadding = 10, \n    groupColors = hcl.colors(n = 30, palette = \"Berlin\"),\n            margin = 90, \n            showGroupnames = T\n  )\n\n\n\n\n\nLet us single out a region. for this case I choose Mara region and see\n\nchangua.mkoa = majina[-1]\ni =9\n\n\n\nmtoko = mkoa.mkoa %>% \n  filter(mikoa == changua.mkoa[i]) %>% \n  dplyr::select(-changua.mkoa[i]) %>%\n  column_to_rownames(var = 'mikoa') %>% \n  as.matrix() \n\n\nmtoko %>% \n  chorddiag(\n    type = \"bipartite\", \n    showTicks = F, \n    tickInterval = 2,\n    groupnameFontsize = 14, \n    groupnamePadding = 10, \n    groupColors = hcl.colors(n = 27, palette = \"Zissou 1\"),\n    margin = 90, \n    showGroupnames = T\n  )"
  },
  {
    "objectID": "posts/spatialState/index.html",
    "href": "posts/spatialState/index.html",
    "title": "Spatial Data is Maturing in R",
    "section": "",
    "text": "R is particularly powerful for spatial statistical analysis and quantitative researchers in particular may find R more useful than GIS desktop applications\nR is particularly powerful for spatial statistical analysis and quantitative researchers in particular may find R more useful than GIS desktop applications. As data becomes more geographical, there is a growing necessity to make spatial data more accessible and easy to process. While there are plenty of tools out there that can make your life much easier when processing spatial data (e.g. QGIS and ArcMap) using R to conduct spatial analysis can be just as easy. This is especially true if you’re new to some of these packages and don’t feel like reading through all of the documentation to learn the package or, even more tedious, writing hundreds of lines of your own code to do something relatively simple. In this article I discuss a few packages that make common spatial statistics methods easy to perform in R [@bivand2006implementing].\nWe will conduct a high-level assessment of the R packages that are dedicated for spatial analysis. By showing network connection across package dependencies — which packages utilize code from another package to execute a task – we will undertake a high-level assessment of the condition of spatial in R. For comparison, we’ll compare our Analysis of Spatial Data task view to the tidyverse, one of R’s most well-known collections of packages, as well as the venerable Environmetrics task view, which includes numerous environmental analysis tools. To accomplish so, we’ll need to write some R code and install the following packages:\nWe will use the handy CRAN_package_db function from the tools package which conveniently grabs information from the DESCRIPTION file of every package on CRAN and turns it into a dataframe.\nHere we are interested with the package and imports columns, so we will select them and drop the rest from the dataset. Then, we parse clean and tidy the columns in the dataset to make it a little easier to work with:\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "posts/spatialState/index.html#package-connectivity",
    "href": "posts/spatialState/index.html#package-connectivity",
    "title": "Spatial Data is Maturing in R",
    "section": "Package Connectivity",
    "text": "Package Connectivity\nLet’s start with a look at the tidyverse. We can take the unusual step of actually employing a function from the tidyverse package (aptly titled tidyverse_packages), which identifies those packages that are formally part of the tidyverse. To see package connection, we filter for those packages and their imports, convert to tbl_graph, then plot using ggraph:\n\ntidyverse_tbl <- tidied_cran_imports %>% \n  filter(package %in% tidyverse_packages()) %>%\n  filter(imports %in% tidyverse_packages()) %>%\n  as_tbl_graph()\n\n\nggraph(tidyverse_tbl, layout = \"nicely\")  + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = FALSE, check_overlap = TRUE, nudge_y = .12) +\n  theme_void()\n\n\n\n\nMany intersecting lines traverse in all directions, as one might anticipate, because many packages in tidyverse import other packages. As the tidyverse develops, this is to be expected.\n\nenv_packages <- ctv:::.get_pkgs_from_ctv_or_repos(views = \"Environmetrics\") %>% \n  unlist(use.names = FALSE)\n\n\nenv_tbl <- tidied_cran_imports %>%\n  filter(package %in% env_packages) %>%\n  filter(imports %in% env_packages) %>%\n  as_tbl_graph()\n\nenv_tbl %>% \n  ggraph(layout = 'nicely') + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = FALSE, check_overlap = TRUE, nudge_y = .3) +\n  theme_void()\n\n\n\n\nNext, let’s look at the Spatial Analysis task view, where we might not expect to see the same level of connectedness. The infrastructure underlying CRAN task views, the ctv package, (sort of) provides a function to obtain a vector of package names for a given task view, which we can use to make a network plot:\n\nspatial_packages <- ctv:::.get_pkgs_from_ctv_or_repos(views = \"Spatial\") %>% \n  unlist(use.names = FALSE)\n\nWe then pull the packages that are in spatial analysis task view that are found in all packages that are tidied and convert them to ggraph table and plot the network\n\nsp_tbl <- tidied_cran_imports %>%\n  filter(package %in% spatial_packages) %>%\n  filter(imports %in% spatial_packages) %>%\n  as_tbl_graph()\n\nsp_tbl %>% \n  ggraph(layout = 'fr') + \n  geom_edge_link(colour = \"grey\") + \n  geom_node_point(colour=\"lightblue\", size=2) + \n  geom_node_text(aes(label=name), repel=FALSE, check_overlap = TRUE, nudge_y = .2) +  \n  theme_graph()\n\n\n\n\n\nsp_tbl %>% \n  ggraph(layout = 'linear',circular = TRUE) + \n  geom_edge_link(colour = \"grey50\") + \n  geom_node_point()+\n  geom_node_text(aes(label = name), colour = \"black\", size = 3.5, parse = TRUE, repel = TRUE, check_overlap = TRUE) +\n  theme_void()\n\n\n\n\nThere is clearly some connectivity among spatial-related packages, which serves as a reminder that task views on CRAN aren’t the only location where users find packages to use. Some programs, like sf, establish a hub of related packages because they share a package maintainer, while others, like sp, investigate spatial systems using a wide range of spatial packages. The graph below shows the number of downloads of the cranlogs package from the RStudio CRAN mirror over the last year.\n\nkgcount <- cran_downloads(packages = spatial_packages, \n                           from = Sys.Date()-1*365, \n                           to = Sys.Date())\n\n\nkgcount %>%\n  group_by(package) %>%\n  summarise(downloads = sum(count)) %>%\n  filter(downloads >= 450000) %>% \n  arrange(desc(downloads)) %>% \n  hchart(type = \"bar\", hcaes(x = package, y = downloads)) %>% \n  hc_xAxis(title = list(text = \"Mothly downloads\")) %>% \n  hc_yAxis(title = FALSE)"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "This blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Masumbuko Semba",
    "section": "",
    "text": "is the Data Scientist at Ng’ara Analytics. When not innovating on data platforms, Semba enjoys spending time to understand the meaning of life through the length of bible.\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Masumbuko Semba",
    "section": "Education",
    "text": "Education\nUniversity of Dar es Salaam, Dar es Salaam | Tanzania, TZ PhD in Oceanography | Sep 2016 - June 2023\nUniversity of Dar es Salaam, Dar es Salaam | Tanzania, TZ Msc in Marine Sciences | Sep 2009 - June 2011\nUniversity of Dar es Salaam, Dar es Salaam | Tanzania, TZ Bsc in Fisheries | Sep 2004 - June 2007"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Masumbuko Semba",
    "section": "Experience",
    "text": "Experience\nButimba Analytics | Data Scientist | April 2018 - present\nWIOMSA | Associate Researcher | Sep 2007 - April 2013"
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "This tab is dedicated to share some of the the data driven web decision support tools that I have developed. Some of these web I just my personal interest and curiosity to want to understand how changes of enviromental variables can help us grasp the glimpse of their changes. Others apps that I developed for institutions. You are free to ask and share them to your colleagues.\nThis blog is brought to you by https://semba.netlify.app"
  },
  {
    "objectID": "dashboards.html#the-exclusive-economic-zone-tool",
    "href": "dashboards.html#the-exclusive-economic-zone-tool",
    "title": "Dashboards",
    "section": "The Exclusive Economic Zone Tool",
    "text": "The Exclusive Economic Zone Tool"
  },
  {
    "objectID": "dashboards.html#the-pemba-channel-hydrographic-hub",
    "href": "dashboards.html#the-pemba-channel-hydrographic-hub",
    "title": "Dashboards",
    "section": "The Pemba Channel Hydrographic Hub",
    "text": "The Pemba Channel Hydrographic Hub"
  },
  {
    "objectID": "dashboards.html#the-coastal-and-marine-data-visualization-tool",
    "href": "dashboards.html#the-coastal-and-marine-data-visualization-tool",
    "title": "Dashboards",
    "section": "The coastal and Marine Data Visualization tool",
    "text": "The coastal and Marine Data Visualization tool"
  },
  {
    "objectID": "dashboards.html#the-national-environmenal-master-plan",
    "href": "dashboards.html#the-national-environmenal-master-plan",
    "title": "Dashboards",
    "section": "The National Environmenal Master Plan",
    "text": "The National Environmenal Master Plan"
  },
  {
    "objectID": "dashboards.html#the-marine-research-grant-data-visualization-tool",
    "href": "dashboards.html#the-marine-research-grant-data-visualization-tool",
    "title": "Dashboards",
    "section": "The Marine Research Grant Data Visualization Tool",
    "text": "The Marine Research Grant Data Visualization Tool"
  },
  {
    "objectID": "dashboards.html#the-dashboard-for-marine-spatial-planning",
    "href": "dashboards.html#the-dashboard-for-marine-spatial-planning",
    "title": "Dashboards",
    "section": "The Dashboard for Marine Spatial planning",
    "text": "The Dashboard for Marine Spatial planning"
  }
]